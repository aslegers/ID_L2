{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pretrained vector-space model\n",
    "'''\n",
    "\n",
    "#this model is Facebook's FastText (skip-gram)\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/antoi/Documents/UDEM/Article2/ID_L2/vecModels/wiki.fr.vec', binary=False)\n",
    "\n",
    "'''\n",
    "Import and tokenize\n",
    "'''\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from unidecode import unidecode #pour convertir les espaces insecables en caracteres ASCII\n",
    "import codecs #pour un module qui tolere les accents?\n",
    "from nltk import word_tokenize\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords #il existe aussi les stopwords Veronicus*, qui seraient à vérifier parce que cette liste-ci est sévère\n",
    "import numpy\n",
    "import pandas\n",
    "from itertools import combinations, islice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#définitions des fonctions pour la sliding window de distance cosine qui itère sur les listes de mots\n",
    "\n",
    "def sliding_window(seq, n):\n",
    "    \"\"\"\n",
    "    Returns a sliding window (of width n) over data from the iterable\n",
    "    s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...\n",
    "    \"\"\"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result\n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "def window_distance(wordVector, window):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        wordVector: your word vector model\n",
    "        window: a list of words\n",
    "    Returns:\n",
    "        a numpy array with all the possible similarities between each 2 words\n",
    "        a set of words missing from the word vector space\n",
    "    \"\"\"\n",
    "    rsl = []\n",
    "    missingWords = set()\n",
    "    for comb in combinations(window, 2):\n",
    "        try:\n",
    "            rsl.append(wordVector.wv.distance(comb[0], comb[1])) #could switch from similarity to distance\n",
    "        except KeyError:\n",
    "            rsl.append(numpy.nan)\n",
    "            # Find missing word from the word vector space\n",
    "            for w in comb:\n",
    "                try:\n",
    "                    wordVector.wv.distance(w, w)\n",
    "                except:\n",
    "                    missingWords.add(w)\n",
    "    return numpy.array(rsl), missingWords\n",
    "def sentence_distance(wordVector, sentence, windowSize):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        wordVector: your word vector model\n",
    "        sentence: a list of words tokenized\n",
    "        windowSize: size of the sliding window\n",
    "    Returns:\n",
    "        a numpy array with all the possible similarities of each windows\n",
    "        a set of words missing from the word vector space\n",
    "    \"\"\"\n",
    "    rsl = []\n",
    "    missingWords = set()\n",
    "    for window in sliding_window(sentence, windowSize):\n",
    "        rslWindow, miss = window_distance(wordVector, window)\n",
    "        rsl.append(rslWindow)\n",
    "        for w in miss:\n",
    "            missingWords.add(w)\n",
    "    return numpy.array(rsl), missingWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToData = os.path.join('C:/', 'Users', 'antoi', 'Documents' , 'UDEM', 'Article2', 'ID_L2', 'data')\n",
    "os.chdir(pathToData)\n",
    "fullSpeech = {}\n",
    "for i in os.listdir(r'C:\\Users\\antoi\\Documents\\UDEM\\Article2\\ID_L2\\data'):\n",
    "    if i.endswith('.txt'):\n",
    "        with open(i, mode='r', encoding='utf-8-sig') as j:\n",
    "            lireSansTiret = j.read().replace('-', ' ') #on remplace manuellement le tiret de 'pique-nique' et 'cerf-volant' parce que notre modele ne prend pas les mots-composes \n",
    "            Tokens = word_tokenize(lireSansTiret) # tokenizer de base de NLTK\n",
    "            TokensASCII = [unidecode(word) for word in Tokens] # retire le mystérieux espace insécable, mais aussi tous les accents\n",
    "            tokens = [word.lower() for word in TokensASCII] # probablement deja en lowercase dans le modele\n",
    "            punctuation = [\",\", \"?\", \"!\", \".\", \"’\",\"'\", \"...\"] # ce n'est pas l'idéal, mais on vire toute la ponctuation manuellement\n",
    "            sample = [m for m in tokens if m not in punctuation] # on retire les signes de poncutation de la liste de strings\n",
    "            fullSpeech[i] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 50\n",
    "csvRowList = []\n",
    "for i, sample in enumerate(fullSpeech):\n",
    "    dist, e = sentence_distance(model, fullSpeech[sample], WINDOW_SIZE)\n",
    "    row = {\n",
    "            \"name\": \"patient{}\".format(sample), \n",
    "            \"distance\": numpy.nanmean(dist), \n",
    "            }\n",
    "    csvRowList.append(row)\n",
    "\n",
    "df = pandas.DataFrame(csvRowList)\n",
    "df = df[[\"name\", \"distance\"]]\n",
    "outputFilename = \"distance.csv\"\n",
    "df.to_csv(outputFilename, sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
